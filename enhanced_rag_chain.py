# enhanced_rag_chain.py

import os
import logging
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
from dotenv import load_dotenv
import langdetect
from vector_store import search_articles

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class EnhancedRAGChain:
    """ê°œì„ ëœ í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.client = client
        self.domain_knowledge_base = self._load_domain_knowledge()
        
    def _load_domain_knowledge(self) -> Dict:
        """í•´ìš´/ì² ê°• ë„ë©”ì¸ ê¸°ë³¸ ì§€ì‹ ë² ì´ìŠ¤"""
        return {
            "vessel_types": {
                "capesize": "180,000 DWT ì´ìƒì˜ ëŒ€í˜• ë²Œí¬ì„ . ì£¼ë¡œ ì² ê´‘ì„, ì„íƒ„ ìš´ì†¡",
                "panamax": "65,000-80,000 DWT ë²Œí¬ì„ . íŒŒë‚˜ë§ˆ ìš´í•˜ í†µê³¼ ê°€ëŠ¥í•œ ìµœëŒ€ í¬ê¸°",
                "supramax": "50,000-65,000 DWT ë²Œí¬ì„ . ì¤‘ê°„ ê·œëª¨ í™”ë¬¼ ìš´ì†¡",
                "handysize": "10,000-40,000 DWT ì†Œí˜• ë²Œí¬ì„ . ì†Œê·œëª¨ í•­ë§Œ ì ‘ê·¼ ê°€ëŠ¥"
            },
            "market_indices": {
                "bdi": "Baltic Dry Index - ê±´í™”ë¬¼ì„  ì¢…í•© ìš´ì„ ì§€ìˆ˜",
                "scfi": "Shanghai Containerized Freight Index - ìƒí•˜ì´ ì»¨í…Œì´ë„ˆ ìš´ì„ ì§€ìˆ˜"
            },
            "commodities": {
                "iron_ore": "ì² ê´‘ì„ - ì œì²  ì›ë£Œ, ì£¼ìš” ìˆ˜ì¶œêµ­: í˜¸ì£¼, ë¸Œë¼ì§ˆ",
                "coal": "ì„íƒ„ - ë°œì „/ì œì² ìš©, ì£¼ìš” ìˆ˜ì¶œêµ­: í˜¸ì£¼, ì¸ë„ë„¤ì‹œì•„",
                "grain": "ê³¡ë¬¼ - ë°€, ì˜¥ìˆ˜ìˆ˜, ëŒ€ë‘ ë“±"
            }
        }
    
    def analyze_query_type(self, query: str) -> Dict:
        """ì§ˆë¬¸ ìœ í˜• ë¶„ì„"""
        query_lower = query.lower()
        
        analysis = {
            "needs_realtime_data": False,
            "needs_domain_knowledge": False,
            "needs_market_analysis": False,
            "query_type": "general",
            "confidence_threshold": 0.7
        }
        
        # ì‹¤ì‹œê°„ ë°ì´í„°ê°€ í•„ìš”í•œ ì§ˆë¬¸
        realtime_keywords = ['ìµœê·¼', 'í˜„ì¬', 'ì˜¤ëŠ˜', 'ì´ë²ˆì£¼', 'ì§€ê¸ˆ', 'recent', 'current', 'today']
        if any(keyword in query_lower for keyword in realtime_keywords):
            analysis["needs_realtime_data"] = True
            analysis["query_type"] = "realtime"
        
        # ê¸°ë³¸ ë„ë©”ì¸ ì§€ì‹ì´ í•„ìš”í•œ ì§ˆë¬¸
        domain_keywords = ['ë­ì•¼', 'ë¬´ì—‡', 'ì„¤ëª…', 'ì°¨ì´', 'what is', 'explain', 'difference']
        if any(keyword in query_lower for keyword in domain_keywords):
            analysis["needs_domain_knowledge"] = True
            analysis["query_type"] = "definition"
        
        # ì‹œì¥ ë¶„ì„ì´ í•„ìš”í•œ ì§ˆë¬¸
        analysis_keywords = ['ì „ë§', 'ì˜ˆì¸¡', 'ë¶„ì„', 'ì˜í–¥', 'forecast', 'predict', 'analysis', 'impact']
        if any(keyword in query_lower for keyword in analysis_keywords):
            analysis["needs_market_analysis"] = True
            analysis["query_type"] = "analysis"
            analysis["confidence_threshold"] = 0.8  # ë” ë†’ì€ ì‹ ë¢°ë„ ìš”êµ¬
        
        return analysis
    
    def search_domain_knowledge(self, query: str) -> str:
        """ë„ë©”ì¸ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰"""
        query_lower = query.lower()
        relevant_info = []
        
        # ì„ ë°• ìœ í˜• ê²€ìƒ‰
        for vessel_type, description in self.domain_knowledge_base["vessel_types"].items():
            if vessel_type in query_lower:
                relevant_info.append(f"**{vessel_type.title()}**: {description}")
        
        # ì‹œì¥ ì§€ìˆ˜ ê²€ìƒ‰
        for index_name, description in self.domain_knowledge_base["market_indices"].items():
            if index_name in query_lower or index_name.upper() in query.upper():
                relevant_info.append(f"**{index_name.upper()}**: {description}")
        
        # ì›ìì¬ ê²€ìƒ‰
        for commodity, description in self.domain_knowledge_base["commodities"].items():
            if commodity.replace('_', ' ') in query_lower:
                relevant_info.append(f"**{commodity.replace('_', ' ').title()}**: {description}")
        
        return "\n".join(relevant_info) if relevant_info else ""
    
    def build_enhanced_answer(self, query: str, user_meta: Dict) -> Tuple[str, Dict]:
        """ê°œì„ ëœ ë‹µë³€ ìƒì„±"""
        
        # 1) ì§ˆë¬¸ ìœ í˜• ë¶„ì„
        query_analysis = self.analyze_query_type(query)
        logging.info(f"ì§ˆë¬¸ ë¶„ì„: {query_analysis}")
        
        # 2) ë²¡í„° ê²€ìƒ‰ (í•­ìƒ ì‹¤í–‰)
        vector_results = search_articles(
            query, 
            filters=user_meta.get("filters", {}), 
            top_k=5
        )
        
        # 3) ë„ë©”ì¸ ì§€ì‹ ê²€ìƒ‰
        domain_info = ""
        if query_analysis["needs_domain_knowledge"]:
            domain_info = self.search_domain_knowledge(query)
        
        # 4) ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼
        if vector_results:
            context_parts.append("[ìµœì‹  ë‰´ìŠ¤ ì •ë³´]")
            for i, result in enumerate(vector_results, 1):
                context_parts.append(f"{i}. ì œëª©: {result['title']}")
                context_parts.append(f"   ìš”ì•½: {result['summary']}")
                context_parts.append(f"   ì¶œì²˜: {result['source_url']}")
                context_parts.append("")
        
        # ë„ë©”ì¸ ì§€ì‹
        if domain_info:
            context_parts.append("[ê¸°ë³¸ ì§€ì‹ ì •ë³´]")
            context_parts.append(domain_info)
            context_parts.append("")
        
        # 5) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = self._build_system_prompt(user_meta, query_analysis)
        user_prompt = self._build_user_prompt(query, context_parts, query_analysis)
        
        # 6) GPT í˜¸ì¶œ
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3
            )
            
            answer = response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"GPT í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # 7) ì¶œì²˜ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
        metadata = {
            "query_type": query_analysis["query_type"],
            "vector_results_count": len(vector_results),
            "has_domain_knowledge": bool(domain_info),
            "confidence": self._calculate_confidence(vector_results, domain_info, query_analysis)
        }
        
        # 8) ì¶œì²˜ ì •ë³´ ì¶”ê°€
        if vector_results:
            sources = [f"- {r['title']} ({r['source']}) â†’ {r['source_url']}" for r in vector_results]
            answer += "\n\n**ğŸ“° ê´€ë ¨ ê¸°ì‚¬:**\n" + "\n".join(sources)
        
        # ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš° ê²½ê³  ì¶”ê°€
        if metadata["confidence"] < query_analysis["confidence_threshold"]:
            answer += f"\n\nâš ï¸ *ì´ ë‹µë³€ì€ ì œí•œëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.*"
        
        return answer, metadata
    
    def _build_system_prompt(self, user_meta: Dict, query_analysis: Dict) -> str:
        """ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        base_prompt = f"""ë‹¹ì‹ ì€ í•´ìš´/ë¬¼ë¥˜/ì² ê°• ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ì ì§ì±…: {user_meta.get('role', 'ë‹´ë‹¹ì')}
ì†Œì† ê·¸ë£¹: {', '.join(user_meta.get('groups', ['general']))}

ë‹µë³€ ì§€ì¹¨:
"""
        
        # ì§ì±…ë³„ ë‹µë³€ ê¹Šì´
        role = user_meta.get('role', 'ë‹´ë‹¹ì')
        if role in ['ì‚¬ì¥', 'ì‹¤ì¥']:
            base_prompt += "- ì „ëµì  ê´€ì ì—ì„œ í•µì‹¬ ìš”ì  ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€\n"
        elif role == 'ê·¸ë£¹ì¥':
            base_prompt += "- ê´€ë¦¬ì  ê´€ì ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ\n"
        elif role == 'ë¦¬ë”':
            base_prompt += "- ì‹¤ë¬´ì§„ì„ ìœ„í•œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ì„¤ëª… í¬í•¨\n"
        else:  # ë‹´ë‹¹ì
            base_prompt += "- ìµœëŒ€í•œ ìì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ì„¤ëª…\n"
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ì§€ì¹¨
        if query_analysis["query_type"] == "realtime":
            base_prompt += "- ìµœì‹  ì •ë³´ ìš°ì„ , ì‹œì  ëª…í™•íˆ í‘œê¸°\n"
        elif query_analysis["query_type"] == "definition":
            base_prompt += "- ê¸°ë³¸ ê°œë…ë¶€í„° ì°¨ê·¼ì°¨ê·¼ ì„¤ëª…\n"
        elif query_analysis["query_type"] == "analysis":
            base_prompt += "- ë‹¤ê°ë„ ë¶„ì„, ìœ„í—˜ ìš”ì†Œ ë° ê¸°íšŒ ìš”ì†Œ ëª¨ë‘ ì–¸ê¸‰\n"
        
        base_prompt += """
ì¤‘ìš” ì›ì¹™:
1. ì œê³µëœ ìµœì‹  ë‰´ìŠ¤ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©
2. ì¶œì²˜ê°€ ë¶ˆë¶„ëª…í•œ ì •ë³´ëŠ” ì¶”ì¸¡ì„± ë‹µë³€ì„ì„ ëª…ì‹œ
3. ì „ë¬¸ ìš©ì–´ ì‚¬ìš©ì‹œ ê°„ë‹¨í•œ ì„¤ëª… ë³‘í–‰
4. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€
"""
        
        return base_prompt
    
    def _build_user_prompt(self, query: str, context_parts: List[str], query_analysis: Dict) -> str:
        """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ì–¸ì–´ ê°ì§€
        try:
            lang = langdetect.detect(query)
            lang_instruction = "[í•œêµ­ì–´ë¡œ ë‹µë³€]" if lang == 'ko' else "[English Response]"
        except:
            lang_instruction = "[í•œêµ­ì–´ë¡œ ë‹µë³€]"
        
        prompt = f"""{lang_instruction}

ì§ˆë¬¸: {query}

"""
        
        # ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if context_parts:
            prompt += "ì°¸ê³  ì •ë³´:\n" + "\n".join(context_parts) + "\n"
        else:
            prompt += "ì°¸ê³  ì •ë³´: ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ í•´ìš´/ì² ê°• ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n"
        
        # íŠ¹ë³„ ì§€ì¹¨
        if query_analysis["query_type"] == "analysis":
            prompt += "\në¶„ì„ ìš”ì²­: ê°€ëŠ¥í•œ ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ì™€ ë¦¬ìŠ¤í¬ ìš”ì†Œë¥¼ ê³ ë ¤í•˜ì—¬ ê· í˜•ì¡íŒ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."
        
        return prompt
    
    def _calculate_confidence(self, vector_results: List, domain_info: str, query_analysis: Dict) -> float:
        """ë‹µë³€ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 0.3  # ê¸°ë³¸ ì‹ ë¢°ë„
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ì¦ê°€
        if vector_results:
            confidence += 0.4
            # ê´€ë ¨ì„±ì´ ë†’ì€ ê²°ê³¼ê°€ ë§ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ì¦ê°€
            confidence += min(len(vector_results) * 0.1, 0.2)
        
        # ë„ë©”ì¸ ì§€ì‹ì´ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ì¦ê°€
        if domain_info:
            confidence += 0.2
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ì¡°ì •
        if query_analysis["query_type"] == "definition":
            confidence += 0.1  # ì •ì˜ ì§ˆë¬¸ì€ ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì 
        elif query_analysis["query_type"] == "realtime":
            if not vector_results:
                confidence -= 0.2  # ì‹¤ì‹œê°„ ì •ë³´ ì—†ìœ¼ë©´ ì‹ ë¢°ë„ í•˜ë½
        
        return min(confidence, 1.0)

# ê¸°ì¡´ í•¨ìˆ˜ ëŒ€ì²´
def build_answer(query: str, user_meta: Dict) -> str:
    """ê¸°ì¡´ í•¨ìˆ˜ì™€ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤"""
    enhanced_rag = EnhancedRAGChain()
    answer, metadata = enhanced_rag.build_enhanced_answer(query, user_meta)
    
    # ë¡œê¹…
    logging.info(f"ë‹µë³€ ë©”íƒ€ë°ì´í„°: {metadata}")
    
    return answer

# ê³ ê¸‰ ë‹µë³€ í•¨ìˆ˜ (ë©”íƒ€ë°ì´í„° í¬í•¨)
def build_enhanced_answer(query: str, user_meta: Dict) -> Tuple[str, Dict]:
    """ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ê³ ê¸‰ ë‹µë³€ ìƒì„±"""
    enhanced_rag = EnhancedRAGChain()
    return enhanced_rag.build_enhanced_answer(query, user_meta)