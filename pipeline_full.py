# pipeline_full.py
"""
ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: í¬ë¡¤ë§ â†’ ë¶„ì„ â†’ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥
"""
import os
import sys
import logging
import json
from datetime import datetime

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ PYTHONPATHì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from crawler_utils import crawl_tradewinds, crawl_freightwaves
from analyzer import analyze_article
from vector_store import add_documents
from config import MAX_ARTICLES_PER_DAY

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/pipeline.log'),
        logging.StreamHandler()
    ]
)

def step1_crawl():
    """1ë‹¨ê³„: ë‰´ìŠ¤ í¬ë¡¤ë§"""
    logging.info("="*60)
    logging.info("1ë‹¨ê³„: ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
    logging.info("="*60)
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    tradewinds_data = crawl_tradewinds(MAX_ARTICLES_PER_DAY)
    freightwaves_data = crawl_freightwaves(MAX_ARTICLES_PER_DAY)
    
    all_articles = tradewinds_data + freightwaves_data
    
    logging.info(f"í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
    logging.info(f"- TradeWinds: {len(tradewinds_data)}ê°œ")
    logging.info(f"- FreightWaves: {len(freightwaves_data)}ê°œ")
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    os.makedirs("data", exist_ok=True)
    crawl_file = "data/crawled_articles.json"
    
    with open(crawl_file, "w", encoding="utf-8") as f:
        json.dump(all_articles, f, ensure_ascii=False, indent=2)
    
    logging.info(f"í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥: {crawl_file}")
    return all_articles

def step2_analyze(articles):
    """2ë‹¨ê³„: GPT ë¶„ì„"""
    logging.info("="*60)
    logging.info("2ë‹¨ê³„: GPT ë¶„ì„ ì‹œì‘")
    logging.info("="*60)
    
    if not articles:
        logging.warning("ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    analyzed_articles = []
    
    for i, article in enumerate(articles, 1):
        try:
            logging.info(f"ë¶„ì„ ì¤‘ ({i}/{len(articles)}): {article['title'][:50]}...")
            
            analyzed = analyze_article(article)
            analyzed["date"] = datetime.now().strftime("%Y-%m-%d")
            analyzed_articles.append(analyzed)
            
        except Exception as e:
            logging.error(f"ê¸°ì‚¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            continue
    
    logging.info(f"ë¶„ì„ ì™„ë£Œ: {len(analyzed_articles)}ê°œ ê¸°ì‚¬")
    
    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    analyzed_file = "data/analyzed_articles.json"
    with open(analyzed_file, "w", encoding="utf-8") as f:
        json.dump(analyzed_articles, f, ensure_ascii=False, indent=2)
    
    logging.info(f"ë¶„ì„ ê²°ê³¼ ì €ì¥: {analyzed_file}")
    return analyzed_articles

def step3_vectorize(analyzed_articles):
    """3ë‹¨ê³„: ë²¡í„°ìŠ¤í† ì–´ ì €ì¥"""
    logging.info("="*60)
    logging.info("3ë‹¨ê³„: ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì‹œì‘")
    logging.info("="*60)
    
    if not analyzed_articles:
        logging.warning("ë²¡í„°í™”í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        add_documents(analyzed_articles)
        logging.info(f"ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ: {len(analyzed_articles)}ê°œ ê¸°ì‚¬")
        
    except Exception as e:
        logging.error(f"ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì‹¤íŒ¨: {e}")

def show_summary(articles, analyzed_articles):
    """ì‹¤í–‰ ê²°ê³¼ ìš”ì•½"""
    print("\n" + "="*80)
    print("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
    print("="*80)
    
    print(f"ğŸ“Š í¬ë¡¤ë§: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
    print(f"ğŸ§  ë¶„ì„: {len(analyzed_articles)}ê°œ ê¸°ì‚¬ ë¶„ì„")
    print(f"ğŸ” ë²¡í„°í™”: {len(analyzed_articles)}ê°œ ê¸°ì‚¬ ì €ì¥")
    
    if analyzed_articles:
        # ì¹´í…Œê³ ë¦¬ ë¶„í¬
        category_count = {}
        group_count = {}
        event_count = {}
        
        for article in analyzed_articles:
            # ì¹´í…Œê³ ë¦¬ í†µê³„
            for cat in article.get('category', []):
                category_count[cat] = category_count.get(cat, 0) + 1
            
            # ê·¸ë£¹ í†µê³„  
            for group in article.get('assigned_group', []):
                group_count[group] = group_count.get(group, 0) + 1
                
            # ì´ë²¤íŠ¸ í†µê³„
            for event in article.get('events', []):
                event_count[event] = event_count.get(event, 0) + 1
        
        print(f"\nğŸ“ˆ ë¶„ì„ ê²°ê³¼:")
        print(f"- ìƒìœ„ ì¹´í…Œê³ ë¦¬: {dict(sorted(category_count.items(), key=lambda x: x[1], reverse=True)[:5])}")
        print(f"- ê·¸ë£¹ ë¶„í¬: {group_count}")
        print(f"- ìƒìœ„ ì´ë²¤íŠ¸: {dict(sorted(event_count.items(), key=lambda x: x[1], reverse=True)[:5])}")
    
    print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   streamlit run streamlit_app.py")
    print("="*80)

def main():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    start_time = datetime.now()
    
    try:
        # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("logs", exist_ok=True)
        os.makedirs("data", exist_ok=True)
        os.makedirs("vector_store", exist_ok=True)
        
        logging.info("ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # 1ë‹¨ê³„: í¬ë¡¤ë§
        articles = step1_crawl()
        
        # 2ë‹¨ê³„: ë¶„ì„
        analyzed_articles = step2_analyze(articles)
        
        # 3ë‹¨ê³„: ë²¡í„°í™”
        step3_vectorize(analyzed_articles)
        
        # ê²°ê³¼ ìš”ì•½
        show_summary(articles, analyzed_articles)
        
        end_time = datetime.now()
        duration = end_time - start_time
        logging.info(f"ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration})")
        
    except Exception as e:
        logging.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise

if __name__ == "__main__":
    main()